# Project Structure

```
sentiment-trading-analysis/
â”‚
â”œâ”€â”€ README.md                    # Main project documentation
â”œâ”€â”€ QUICK_START.md              # Quick setup guide
â”œâ”€â”€ GITHUB_SETUP.md             # GitHub upload instructions
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”‚
â”œâ”€â”€ main_1.ipynb                # Main Jupyter notebook with full pipeline
â”‚
â”œâ”€â”€ docs/                       # Documentation folder
â”‚   â””â”€â”€ CE-901_Report__1_.pdf  # Full MSc thesis (39 pages)
â”‚
â””â”€â”€ (future additions)
    â”œâ”€â”€ src/                    # Source code modules (when refactoring)
    â”‚   â”œâ”€â”€ data_collection.py
    â”‚   â”œâ”€â”€ sentiment_analysis.py
    â”‚   â”œâ”€â”€ feature_engineering.py
    â”‚   â”œâ”€â”€ model_training.py
    â”‚   â””â”€â”€ backtesting.py
    â”‚
    â”œâ”€â”€ data/                   # Data storage (gitignored)
    â”‚   â”œâ”€â”€ raw/
    â”‚   â””â”€â”€ processed/
    â”‚
    â”œâ”€â”€ models/                 # Saved model checkpoints
    â”‚
    â”œâ”€â”€ results/                # Output plots and metrics
    â”‚
    â””â”€â”€ tests/                  # Unit tests
        â””â”€â”€ test_sentiment.py

```

## ğŸ“ File Descriptions

### Root Files

- **README.md**: Comprehensive project overview with results, architecture, installation, and usage instructions
- **QUICK_START.md**: Fast-track guide to run the project in under 10 minutes
- **GITHUB_SETUP.md**: Step-by-step instructions for uploading to GitHub
- **LICENSE**: MIT License for open-source distribution
- **requirements.txt**: All Python package dependencies
- **.gitignore**: Files and directories to exclude from version control

### Main Code

- **main_1.ipynb**: Complete ML pipeline including:
  - Cell 1: Dependency installation
  - Cell 2: Imports and API configuration
  - Cell 3: Ticker configuration
  - Cell 4: FinBERT initialization
  - Cell 5: Helper functions (text cleaning, simulation)
  - Cell 6-15: Data collection and preprocessing
  - Cell 16-20: Feature engineering
  - Cell 21-25: Model training and evaluation
  - Cell 26-30: Backtesting framework
  - Cell 31-35: SHAP explainability
  - Cell 36-40: Visualization and results

### Documentation

- **docs/CE-901_Report__1_.pdf**: Full academic thesis (39 pages) including:
  - Literature review (27 citations)
  - Methodology
  - Results and analysis
  - Discussion
  - Conclusions
  - Appendices with detailed tables

## ğŸ”„ Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA COLLECTION                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Yahoo Financeâ”‚  â”‚Social Media  â”‚  â”‚Financial Newsâ”‚  â”‚
â”‚  â”‚  (yfinance)  â”‚  â”‚(Twitter/Redditâ”‚  â”‚  (NewsAPI)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          v                  v                  v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PREPROCESSING                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â€¢ Text cleaning (URLs, mentions, hashtags)        â”‚ â”‚
â”‚  â”‚  â€¢ Tokenization                                    â”‚ â”‚
â”‚  â”‚  â€¢ Feature scaling and normalization               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 FEATURE ENGINEERING                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Sentiment Analysis   â”‚  â”‚ Technical Indicators     â”‚â”‚
â”‚  â”‚ â€¢ VADER (lexicon)    â”‚  â”‚ â€¢ MA5 (moving avg)       â”‚â”‚
â”‚  â”‚ â€¢ FinBERT (DL)       â”‚  â”‚ â€¢ RSI14 (momentum)       â”‚â”‚
â”‚  â”‚ â€¢ Combined score     â”‚  â”‚ â€¢ ret1, ret5 (returns)   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL TRAINING                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Logistic Regression                               â”‚ â”‚
â”‚  â”‚  â€¢ Input: Sentiment + Technical features          â”‚ â”‚
â”‚  â”‚  â€¢ Output: BUY (1) / HOLD (0) probabilities      â”‚ â”‚
â”‚  â”‚  â€¢ Walk-forward validation                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SIGNAL GENERATION                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Confidence-Based Filtering                        â”‚ â”‚
â”‚  â”‚  â€¢ Conservative: prob â‰¥ 0.65 â†’ BUY                â”‚ â”‚
â”‚  â”‚  â€¢ Moderate: prob â‰¥ 0.60 â†’ BUY                    â”‚ â”‚
â”‚  â”‚  â€¢ Aggressive: prob â‰¥ 0.55 â†’ BUY                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKTESTING                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Strategy Execution & Evaluation                   â”‚ â”‚
â”‚  â”‚  â€¢ Returns: Total, Buy-and-Hold comparison        â”‚ â”‚
â”‚  â”‚  â€¢ Risk: Sharpe ratio, Max drawdown               â”‚ â”‚
â”‚  â”‚  â€¢ Trade analysis: Win rate, # of trades          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EXPLAINABILITY                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  SHAP (SHapley Additive exPlanations)             â”‚ â”‚
â”‚  â”‚  â€¢ Feature importance ranking                      â”‚ â”‚
â”‚  â”‚  â€¢ Individual prediction explanations             â”‚ â”‚
â”‚  â”‚  â€¢ Visualization of model decisions                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Key Components

### 1. Data Collection Module
- **Purpose**: Fetch market prices and social sentiment
- **Technologies**: yfinance, tweepy, praw, NewsAPI
- **Output**: Pandas DataFrames with timestamps

### 2. Sentiment Analysis Module
- **Purpose**: Score text sentiment from -1 (negative) to +1 (positive)
- **Technologies**: VADER (rule-based), FinBERT (transformer-based)
- **Output**: Aggregated sentiment scores per day per ticker

### 3. Feature Engineering Module
- **Purpose**: Create predictive features combining sentiment + technical indicators
- **Technologies**: pandas, numpy
- **Output**: Feature matrix for ML model

### 4. Model Training Module
- **Purpose**: Train classifier to predict next-day price movements
- **Technologies**: scikit-learn (LogisticRegression)
- **Output**: Trained model with probability outputs

### 5. Backtesting Module
- **Purpose**: Simulate historical trading to evaluate strategy
- **Technologies**: Custom Python implementation
- **Output**: Performance metrics (returns, Sharpe, drawdown)

### 6. Explainability Module
- **Purpose**: Understand which features drive predictions
- **Technologies**: SHAP
- **Output**: Feature importance plots and values

## ğŸ“Š Data Flow

```
Raw Data â†’ Cleaned Data â†’ Engineered Features â†’ Model â†’ Predictions â†’ Backtested Returns
   â†“                                               â†“
Sentiment                                      SHAP Values
(text â†’ score)                              (interpretability)
```

## ğŸ”§ Future Modularization Plan

When refactoring notebook into production code:

```python
# Example modular structure
from src.data_collection import fetch_market_data, fetch_social_sentiment
from src.sentiment_analysis import analyze_vader, analyze_finbert, combine_sentiment
from src.feature_engineering import create_technical_indicators, merge_features
from src.model_training import train_classifier, generate_signals
from src.backtesting import backtest_strategy, calculate_metrics
from src.explainability import explain_predictions

# Main pipeline
market_data = fetch_market_data(tickers=['AAPL', 'TSLA'], days=180)
sentiment_data = fetch_social_sentiment(tickers=['AAPL', 'TSLA'], days=30)
features = merge_features(market_data, sentiment_data)
model = train_classifier(features)
signals = generate_signals(model, features)
results = backtest_strategy(signals, market_data)
explanations = explain_predictions(model, features)
```

## ğŸ’¡ Usage Patterns

### Quick Experimentation
```bash
jupyter notebook main_1.ipynb
# Modify parameters in-notebook, run cells
```

### Automated Backtesting
```bash
# Future: CLI tool
python run_backtest.py --tickers AAPL TSLA NVDA --days 180 --strategy aggressive
```

### Live Trading (Future)
```bash
# Future: Real-time deployment
python live_trading.py --config config.yaml
```

## ğŸ“ˆ Performance Benchmarks

**Current Setup:**
- Training time: ~2-5 minutes (depending on data volume)
- Inference: <1 second per prediction
- Memory usage: ~500MB (with FinBERT loaded)
- Disk space: ~1GB (model cache + data)

---

**This structure is designed for clarity, reproducibility, and future scalability.** ğŸš€
